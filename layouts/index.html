{{ define "main" }}

<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Montserrat:400,400i,500,500i,700,700i|Noto+Sans:400,400i,700,700i|Source+Code+Pro&amp;subset=latin-ext">

<section class="section hero-subheader container-fluid mt-n3 pb-3">

  <div class="container">
    <!-- First Row -->
      <div class="row justify-content-center">
            <div class="col-lg-12 text-center">
              <h1 class="mt-0">{{ .Title }}</h1>
            </div>

        <div class="col-lg-9 col-xl-8 text-center">
           <p class="lead">{{ .Params.lead | safeHTML }}</p>
        </div>
      </div>

    <!-- Second  Row -->
      <div class="row justify-content-center">
          <div class="col-lg-9 col-xl-8 text-center">
            <a class="btn btn-primary btn-lg px-4 mb-2" href="https://arxiv.org/abs/2308.14089" role="button">Paper</a>
            <a class="btn btn-primary btn-lg px-4 mb-2" href="https://github.com/som-shahlab/medalign/" role="button">GitHub</a>
            <a class="btn btn-primary btn-lg px-4 mb-2" href="https://redivis.com/ShahLab/datasets" role="button">Dataset Access</a>
          </div>
        </div>

        <!-- Third  Row -->
        <div style="text-align: justify; text-justify: inter-word;">
            <p>
                Large language models (LLMs) have demonstrated human-level fluency in following natural language instructions, offering potential to reduce administrative burdens in healthcare. However, evaluating LLMs on real-world clinical tasks remains a challenge.
                <strong>MEDALIGN</strong> addresses this by introducing a benchmark dataset of <strong>983 natural language instructions</strong> for Electronic Health Record (EHR) data, curated by <strong>15 clinicians across 7 specialties</strong>. The dataset includes:
            </p>
            <ul>
                <li><strong>302 clinician-written reference responses</strong> for instruction-following evaluation.</li>
                <li><strong>275 longitudinal EHRs</strong> to ground instruction-response pairs.</li>
                <li>Comparative analysis of <strong>six LLMs</strong>, revealing high error rates ranging from <strong>35% (GPT-4) to 68% (MPT-7B-Instruct)</strong>.</li>
                <li>An <strong>8.3% drop in accuracy</strong> when GPT-4 moved from a 32k to a 2k context length.</li>
                <li>Assessment of correlations between clinician rankings and automated metrics, highlighting <strong>COMET as the best-performing automated evaluation metric</strong>.</li>
            </ul>
        </div>

        <!-- Fourth Row -->
        <div class="row justify-content-center d-md-block">
          <img src="https://raw.githubusercontent.com/som-shahlab/medalign-website/main/images/medalign_workflow-1.png" alt="MEDALIGN Workflow"  class="border-0" >
        </div>
  </div>
</section>
{{ end }}

{{ define "sidebar-prefooter" }}
<div class="container">
<div class="row justify-content-center">
  <h3>Citation</h3>
  <pre id="arxiv-citation">
@inproceedings{DBLP:conf/aaai/FlemingLHJRTBGS24,
  author       = {Scott L. Fleming and Alejandro Lozano and William J. Haberkorn and Jenelle A. Jindal and Eduardo Reis and Rahul Thapa and Louis Blankemeier and Julian Z. Genkins and Ethan Steinberg and Ashwin Nayak and Birju S. Patel and Chia{-}Chun Chiang and Alison Callahan and Zepeng Huo and Sergios Gatidis and Scott J. Adams and Oluseyi Fayanju and Shreya J. Shah and Thomas Savage and Ethan Goh and Akshay S. Chaudhari and Nima Aghaeepour and Christopher D. Sharp and Michael A. Pfeffer and Percy Liang and Jonathan H. Chen and Keith E. Morse and Emma P. Brunskill and Jason A. Fries and Nigam H. Shah},
  title        = {MedAlign: {A} Clinician-Generated Dataset for Instruction Following with Electronic Medical Records},
  booktitle    = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence},
  year         = {2024},
  url          = {https://doi.org/10.1609/aaai.v38i20.30205},
  doi          = {10.1609/AAAI.V38I20.30205},
}
</pre>
</div>
</div>
{{ end }}
